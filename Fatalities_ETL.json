{
	"jobConfig": {
		"name": "Fatalities_ETL",
		"description": "",
		"role": "arn:aws:iam::484907516017:role/AWSGlueServiceRole_socialmedia",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "Fatalities_ETL.py",
		"scriptLocation": "s3://aws-glue-assets-484907516017-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-08-25T11:10:38.390Z",
		"developerMode": true,
		"connectionsList": [
			"nk Jdbc connection"
		],
		"temporaryDirectory": "s3://aws-glue-assets-484907516017-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-484907516017-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"logging": false,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.dynamicframe import DynamicFrame\r\nfrom pyspark.sql.functions import col, trim, initcap, to_date, when\r\nfrom pyspark.sql.types import DoubleType, StringType, DateType\r\nimport boto3\r\n\r\n# ----------------- Glue boilerplate -----------------\r\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\r\nsc = SparkContext()\r\nglue = GlueContext(sc)\r\nspark = glue.spark_session\r\njob = Job(glue); job.init(args[\"JOB_NAME\"], args)\r\n\r\n# ----------------- Config -----------------\r\nRAW_PATH = \"s3://sarasocialmediabucket/Final-Project/raw/Fatalities Isr-Pse Conflict 2000-2023.csv\"\r\nTMP_DIR = \"s3://sarasocialmediabucket/Final-Project/temp/\"\r\nCATALOG_CONN = \"nk Jdbc connection\"\r\nTARGET_DB = \"dev\"\r\nSTAGING_TABLE = \"contoso.stg_fatalities_dwh\"\r\nFINAL_TABLE = \"contoso.fatalities_dwh\"\r\n\r\n# ------------------------------------------\r\n# 1) Read raw CSV using Spark DataFrame Reader for better control\r\ndf = (spark.read\r\n      .option(\"header\", True)\r\n      .option(\"escape\", '\"')\r\n      .csv(RAW_PATH))\r\n\r\n# 2) Apply transformations\r\ndf_cleaned = (df\r\n    .withColumn(\"age\", col(\"age\").cast(DoubleType()))\r\n    .filter(col(\"age\") <= 100)\r\n    .withColumn(\"gender\", trim(col(\"gender\")).cast(StringType()))\r\n    .withColumn(\"type_of_injury\", trim(col(\"type_of_injury\")).cast(StringType()))\r\n    .withColumn(\"killed_by\", trim(col(\"killed_by\")).cast(StringType()))\r\n    .withColumn(\"took_part_in_the_hostilities\", trim(col(\"took_part_in_the_hostilities\")).cast(StringType()))\r\n)\r\n\r\n# Fill missing values\r\nmedian_age = df_cleaned.approxQuantile(\"age\", [0.5], 0.001)[0]\r\nmode_gender = df_cleaned.groupBy(\"gender\").count().orderBy(\"count\", ascending=False).first()[0]\r\nmode_injury = df_cleaned.groupBy(\"type_of_injury\").count().orderBy(\"count\", ascending=False).first()[0]\r\n\r\ndf_cleaned = (df_cleaned\r\n    .fillna({\"age\": median_age})\r\n    .fillna({\"gender\": mode_gender})\r\n    .fillna({\"type_of_injury\": mode_injury})\r\n    .fillna({\"took_part_in_the_hostilities\": \"Unknown\"})\r\n)\r\n\r\n# Parse dates - using a safe way to handle both yyyy-MM-dd and MM/dd/yyyy if they appear\r\ndf_cleaned = (df_cleaned\r\n    .withColumn(\"date_of_event\",\r\n        when(to_date(col(\"date_of_event\"), \"yyyy-MM-dd\").isNotNull(), to_date(col(\"date_of_event\"), \"yyyy-MM-dd\"))\r\n        .otherwise(to_date(col(\"date_of_event\"), \"MM/dd/yyyy\"))\r\n        .cast(DateType())\r\n    )\r\n    .withColumn(\"date_of_death\",\r\n        when(to_date(col(\"date_of_death\"), \"yyyy-MM-dd\").isNotNull(), to_date(col(\"date_of_death\"), \"yyyy-MM-dd\"))\r\n        .otherwise(to_date(col(\"date_of_death\"), \"MM/dd/yyyy\"))\r\n        .cast(DateType())\r\n    )\r\n)\r\n\r\n# Cast remaining columns to correct types\r\ndf_cleaned = (df_cleaned\r\n    .withColumn(\"name\", col(\"name\").cast(StringType()))\r\n    .withColumn(\"citizenship\", col(\"citizenship\").cast(StringType()))\r\n    .withColumn(\"event_location\", col(\"event_location\").cast(StringType()))\r\n    .withColumn(\"event_location_district\", col(\"event_location_district\").cast(StringType()))\r\n    .withColumn(\"event_location_region\", col(\"event_location_region\").cast(StringType()))\r\n    .withColumn(\"place_of_residence\", col(\"place_of_residence\").cast(StringType()))\r\n    .withColumn(\"place_of_residence_district\", col(\"place_of_residence_district\").cast(StringType()))\r\n    .withColumn(\"notes\", col(\"notes\").cast(StringType()))\r\n)\r\n\r\n# Handle the `ammunition` column drop\r\nif 'ammunition' in df_cleaned.columns:\r\n    df_cleaned = df_cleaned.drop(\"ammunition\")\r\n\r\n# Drop the `notes` column to avoid potential data size issues with VARCHAR(MAX)\r\nif 'notes' in df_cleaned.columns:\r\n    df_cleaned = df_cleaned.drop(\"notes\")\r\n\r\n\r\n# 3) Write to staging (truncate + load fresh data)\r\ndyf = DynamicFrame.fromDF(df_cleaned, glue, \"dyf\")\r\nglue.write_dynamic_frame.from_jdbc_conf(\r\n    frame=dyf,\r\n    catalog_connection=CATALOG_CONN,\r\n    connection_options={\r\n        \"dbtable\": STAGING_TABLE,\r\n        \"database\": TARGET_DB,\r\n        \"preactions\": f\"TRUNCATE TABLE {STAGING_TABLE};\"\r\n    },\r\n    redshift_tmp_dir=TMP_DIR\r\n)\r\n\r\n# 4) Upsert using DELETE + INSERT (compatible with old Redshift)\r\nupsert_sql = f\"\"\"\r\nBEGIN;\r\n\r\nDELETE FROM {FINAL_TABLE}\r\nWHERE \"name\" IN (SELECT \"name\" FROM {STAGING_TABLE});\r\n\r\nINSERT INTO {FINAL_TABLE} (\r\n    name, age, gender, citizenship, event_location, event_location_district, event_location_region,\r\n    place_of_residence, place_of_residence_district, type_of_injury, killed_by,\r\n    took_part_in_the_hostilities, date_of_event, date_of_death\r\n)\r\nSELECT\r\n    name, age, gender, citizenship, event_location, event_location_district, event_location_region,\r\n    place_of_residence, place_of_residence_district, type_of_injury, killed_by,\r\n    took_part_in_the_hostilities, date_of_event, date_of_death\r\nFROM {STAGING_TABLE};\r\n\r\nCOMMIT;\r\n\"\"\"\r\n\r\n# Execute the upsert using preactions on a dummy write\r\ntry:\r\n    glue.write_dynamic_frame.from_jdbc_conf(\r\n        frame=DynamicFrame.fromDF(spark.range(1).limit(0), glue, \"noop\"),\r\n        catalog_connection=CATALOG_CONN,\r\n        connection_options={\r\n            \"dbtable\": FINAL_TABLE,\r\n            \"database\": TARGET_DB,\r\n            \"preactions\": upsert_sql\r\n        },\r\n        redshift_tmp_dir=TMP_DIR\r\n    )\r\n    print(\"Upsert operation completed successfully\")\r\nexcept Exception as e:\r\n    print(f\"Error during upsert: {str(e)}\")\r\n    raise\r\n\r\njob.commit()"
}